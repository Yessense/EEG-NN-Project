{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils\n",
    "import features\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "import classifier\n",
    "\n",
    "# Для отображения графиков\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# data folder\n",
    "data_folder = './data/'\n",
    "file_name = 'concentrate_t.csv'\n",
    "model_folder = './model/'\n",
    "\n",
    "# constants\n",
    "SEED = 1\n",
    "DELTA_TIME = 128\n",
    "CHUNK_SIZE = 8\n",
    "\n",
    "# ---------------------------------------\n",
    "# Hyperparameters\n",
    "# ---------------------------------------\n",
    "\n",
    "SHIFT_TRAIN = 34\n",
    "SHIFT_TEST = 64\n",
    "\n",
    "dropout = 0.3\n",
    "lr = 0.001\n",
    "batch_size = 256\n",
    "\n",
    "# ---------------------------------------\n",
    "# remarks\n",
    "'''\n",
    "dropout = 0.2, lr = 0.001 is good 90%\n",
    "'''\n",
    "verbose = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reading file and split for test/train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file ./data/concentrate_t.csv \n",
      "\n",
      "Creating train/test...\n",
      "train.shape (51200, 16)\n",
      "test.shape (13312, 16)\n",
      "X_test.shape (195, 128, 14)\n",
      "Y_test.shape (195,)\n",
      "X_train.shape (1320, 128, 14)\n",
      "Y_train.shape (1320,)\n"
     ]
    }
   ],
   "source": [
    "if verbose == 1:\n",
    "    print('Reading file', data_folder + file_name, '\\n')\n",
    "\n",
    "# reading file\n",
    "data = pd.read_csv(data_folder + file_name)\n",
    "data['class'], class_dict = utils.encode_column(data['class'])\n",
    "\n",
    "if verbose == 1:\n",
    "    print('Creating train/test...')\n",
    "\n",
    "# train test split\n",
    "train, test = utils.eeg_train_test_split(data.to_numpy(), chunk_size=DELTA_TIME * CHUNK_SIZE,\n",
    "                                         test_size=0.2, random_state=SEED)\n",
    "\n",
    "if verbose == 1:\n",
    "    print('train.shape', train.shape)\n",
    "    print('test.shape', test.shape)\n",
    "\n",
    "# creating 3d array from 2d\n",
    "X_test, Y_test = utils.create_x_y(test, dt=DELTA_TIME, shift=SHIFT_TEST, verbose=0)\n",
    "X_train, Y_train = utils.create_x_y(train, dt=DELTA_TIME, shift=SHIFT_TRAIN)\n",
    "\n",
    "if verbose == 1:\n",
    "    print('X_test.shape', X_test.shape)\n",
    "    print('Y_test.shape', Y_test.shape)\n",
    "    print('X_train.shape', X_train.shape)\n",
    "    print('Y_train.shape', Y_train.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing\n",
    "\n",
    "- Scale\n",
    "- FFT\n",
    "- Normalize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_p.shape: (1320, 14, 128)\n",
      "X_train_fft.shape: (1320, 14, 65)\n"
     ]
    }
   ],
   "source": [
    "import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "import torch_utils\n",
    "from joblib import dump\n",
    "\n",
    "preprocess_X = Pipeline(steps=[\n",
    "    ('Scale', preprocessing.Scale()),\n",
    "    ('Normalize', preprocessing.Normalize()),\n",
    "    ('Transpose', preprocessing.Transpose())\n",
    "])\n",
    "\n",
    "preprocess_X_fft = Pipeline(steps=[\n",
    "    ('Scale', preprocessing.Scale()),\n",
    "    ('FFT', preprocessing.FFT()),\n",
    "    ('Normalize', preprocessing.Normalize()),\n",
    "    ('Transpose', preprocessing.Transpose()),\n",
    "])\n",
    "\n",
    "X_train_p = preprocess_X.fit_transform(X_train)\n",
    "X_train_fft = preprocess_X_fft.fit_transform(X_train)\n",
    "dump(preprocess_X, model_folder + 'preprocess_X.joblib')\n",
    "dump(preprocess_X_fft, model_folder + 'preprocess_X_fft.joblib')\n",
    "\n",
    "\n",
    "X_test_p = preprocess_X.transform(X_test)\n",
    "X_test_fft = preprocess_X_fft.transform(X_test)\n",
    "if verbose == 1:\n",
    "    print('X_train_p.shape:', X_train_p.shape)\n",
    "    print('X_train_fft.shape:', X_train_fft.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Torch.\n",
    "\n",
    "Creating datasets, loaders etc."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Creatin datasets...\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if verbose == 1:\n",
    "    print('Device:', device)\n",
    "    print('Creatin datasets...')\n",
    "\n",
    "datasets = torch_utils.create_dataset((X_train_p, X_train_fft), Y_train),\\\n",
    "           torch_utils.create_dataset((X_test_p, X_test_fft), Y_test)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating loaders...\n",
      "Raw feature number: 14\n",
      "FFT feature number: 14\n",
      "Train size: 1320\n",
      "Learning rate: 0.001\n",
      "Max epochs: 3000\n",
      "Iterations per epoch: 6\n",
      "Period: 18000\n",
      "Creating model...\n",
      "Start model training\n",
      "Epoch:   1. Loss: 0.6798. Acc.: 53.85%\n",
      "Epoch 1 best model saved with accuracy: 53.85%\n",
      "Epoch:   2. Loss: 0.6554. Acc.: 53.85%\n",
      "Epoch:   4. Loss: 0.6187. Acc.: 53.85%\n",
      "Epoch:   8. Loss: 0.5437. Acc.: 53.33%\n",
      "Epoch 11 best model saved with accuracy: 55.38%\n",
      "Epoch 12 best model saved with accuracy: 58.46%\n",
      "Epoch 13 best model saved with accuracy: 64.62%\n",
      "Epoch 14 best model saved with accuracy: 70.77%\n",
      "Epoch 15 best model saved with accuracy: 74.87%\n",
      "Epoch:  16. Loss: 0.3860. Acc.: 80.51%\n",
      "Epoch 16 best model saved with accuracy: 80.51%\n",
      "Epoch 17 best model saved with accuracy: 82.05%\n",
      "Epoch 18 best model saved with accuracy: 83.59%\n",
      "Epoch 19 best model saved with accuracy: 86.15%\n",
      "Epoch 20 best model saved with accuracy: 87.18%\n",
      "Epoch 26 best model saved with accuracy: 88.72%\n",
      "Epoch 31 best model saved with accuracy: 89.23%\n",
      "Epoch:  32. Loss: 0.2274. Acc.: 88.21%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-5-1166a0e5a347>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     56\u001B[0m         \u001B[0mepoch_loss\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     57\u001B[0m         \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 58\u001B[1;33m         \u001B[0mopt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     59\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     60\u001B[0m     \u001B[0mepoch_loss\u001B[0m \u001B[1;33m/=\u001B[0m \u001B[0mtrain_size\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     24\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 26\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     27\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mF\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001B[0m in \u001B[0;36mstep\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    106\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    107\u001B[0m             \u001B[0mbeta1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbeta2\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgroup\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'betas'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 108\u001B[1;33m             F.adam(params_with_grad,\n\u001B[0m\u001B[0;32m    109\u001B[0m                    \u001B[0mgrads\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    110\u001B[0m                    \u001B[0mexp_avgs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\functional.py\u001B[0m in \u001B[0;36madam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001B[0m\n\u001B[0;32m     84\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     85\u001B[0m         \u001B[1;31m# Decay the first and second moment running average coefficient\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 86\u001B[1;33m         \u001B[0mexp_avg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmul_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbeta1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrad\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0malpha\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mbeta1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     87\u001B[0m         \u001B[0mexp_avg_sq\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmul_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbeta2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maddcmul_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrad\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mbeta2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     88\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mamsgrad\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if verbose == 1:\n",
    "    print('Creating loaders...')\n",
    "\n",
    "trn_dl, val_dl = torch_utils.create_loaders(datasets, bs=batch_size)\n",
    "raw_feat_number = X_train_p.shape[1]\n",
    "fft_feat_number = X_train_fft.shape[1]\n",
    "train_size = len(Y_train)\n",
    "\n",
    "if verbose == 1:\n",
    "    print('Raw feature number:', raw_feat_number)\n",
    "    print('FFT feature number:', fft_feat_number)\n",
    "    print('Train size:', train_size)\n",
    "\n",
    "\n",
    "n_epochs = 3000\n",
    "iterations_per_epoch = len(trn_dl)\n",
    "period = n_epochs * iterations_per_epoch\n",
    "\n",
    "if verbose == 1:\n",
    "    print('Learning rate:', lr)\n",
    "    print('Max epochs:', n_epochs)\n",
    "    print('Iterations per epoch:', iterations_per_epoch)\n",
    "    print('Period:', period)\n",
    "\n",
    "num_classes = 2\n",
    "patience, trials = 500, 0\n",
    "base = 1\n",
    "step = 2\n",
    "\n",
    "best_acc = 0\n",
    "iteration = 0\n",
    "loss_history = []\n",
    "acc_history = []\n",
    "\n",
    "model = classifier.Classifier(raw_feat_number, fft_feat_number, num_classes, drop = dropout).to(device)\n",
    "\n",
    "if verbose == 1:\n",
    "    print('Creating model...')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "opt = optim.Adam(model.parameters(), lr=lr)\n",
    "sched = classifier.Scheduler(opt, partial(classifier.one_cycle, t_max=period, pivot=0.1))\n",
    "\n",
    "print('Start model training')\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(trn_dl):\n",
    "        iteration += 1\n",
    "        x_raw, x_fft, y_batch = [t.to(device) for t in batch]\n",
    "        sched.step(iteration)  # update the learning rate\n",
    "        opt.zero_grad()\n",
    "        out = model(x_raw, x_fft)\n",
    "        loss = criterion(out, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    epoch_loss /= train_size\n",
    "    loss_history.append(epoch_loss)\n",
    "\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for batch in val_dl:\n",
    "        x_raw, x_fft, y_batch = [t.to(device) for t in batch]\n",
    "        out = model(x_raw, x_fft)\n",
    "        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "\n",
    "    acc = correct / total\n",
    "    acc_history.append(acc)\n",
    "\n",
    "    if epoch % base == 0:\n",
    "        print(f'Epoch: {epoch:3d}. Loss: {epoch_loss:.4f}. Acc.: {acc:2.2%}')\n",
    "        base *= step\n",
    "\n",
    "    if acc > best_acc:\n",
    "        trials = 0\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), model_folder + 'best.pth')\n",
    "        print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= patience:\n",
    "            print(f'Early stopping on epoch {epoch}')\n",
    "            break\n",
    "\n",
    "os.rename(model_folder + 'best.pth', model_folder + f'best {best_acc:0.4f}.pth')\n",
    "print('Done!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def score_model(model, metric, data):\n",
    "    model.eval()  # testing mode\n",
    "    scores = 0\n",
    "    for X_batch, Y_label in data:\n",
    "        Y_pred = model.forward(X_batch.to(device)).float()\n",
    "        scores += metric(Y_pred, Y_label.to(device)).mean().item()\n",
    "\n",
    "    return scores/len(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def smooth(y, box_pts):\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    return y_smooth"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax[0].plot(loss_history, label='loss')\n",
    "ax[0].set_title('Validation Loss History')\n",
    "ax[0].set_xlabel('Epoch no.')\n",
    "ax[0].set_ylabel('Loss')\n",
    "\n",
    "ax[1].plot(smooth(acc_history, 5)[:-2], label='acc')\n",
    "ax[1].set_title('Validation Accuracy History')\n",
    "ax[1].set_xlabel('Epoch no.')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}